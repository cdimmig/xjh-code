{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb4b4367",
   "metadata": {},
   "source": [
    "# some common functions in implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd5fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = None\n",
    "\n",
    "def init_gpu(use_gpu=True, gpu_id=0):\n",
    "    global device\n",
    "    if torch.cuda.is_available() and use_gpu:\n",
    "        device = torch.device(\"cuda:\" + str(gpu_id))\n",
    "        print(\"Using GPU id {}\".format(gpu_id))\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU not detected. Defaulting to CPU.\")\n",
    "\n",
    "def set_device(gpu_id):\n",
    "    torch.cuda.set_device(gpu_id)\n",
    "\n",
    "def from_numpy(*args, **kwargs):\n",
    "    return torch.from_numpy(*args, **kwargs).float().to(device)\n",
    "\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.to('cpu').detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df162b61",
   "metadata": {},
   "source": [
    "# quick code for MLP (no loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import itertools\n",
    "from typing import Any\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7e24b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "Activation = Union[str, nn.Module]\n",
    "\n",
    "_str_to_activation = {\n",
    "    'relu': nn.ReLU(),\n",
    "    'tanh': nn.Tanh(),\n",
    "    'leaky_relu': nn.LeakyReLU(),\n",
    "    'sigmoid': nn.Sigmoid(),\n",
    "    'selu': nn.SELU(),\n",
    "    'softplus': nn.Softplus(),\n",
    "    'identity': nn.Identity(),\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "        Builds a feedforward neural network\n",
    "        arguments:\n",
    "            n_layers: number of hidden layers\n",
    "            size: dimension of each hidden layer\n",
    "            activation: activation of each hidden layer\n",
    "\n",
    "            input_size: size of the input layer\n",
    "            output_size: size of the output layer\n",
    "            output_activation: activation of the output layer\n",
    "\n",
    "        returns:\n",
    "            MLP (nn.Module)\n",
    "\"\"\"\n",
    "def build_mlp(\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        n_layers: int,\n",
    "        size: int,\n",
    "        activation: Activation = 'tanh',\n",
    "        output_activation: Activation = 'identity') -> nn.Module:\n",
    "\n",
    "    if isinstance(activation, str):\n",
    "        activation = _str_to_activation[activation]\n",
    "    if isinstance(output_activation, str):\n",
    "        output_activation = _str_to_activation[output_activation]\n",
    "\n",
    "    # TODO: return a MLP. This should be an instance of nn.Module\n",
    "    # Note: nn.Sequential is an instance of nn.Module.\n",
    "    layers = [('linear1', nn.Linear(input_size, size)), ('activation1', activation)]\n",
    "    for i in range(2, n_layers+1):\n",
    "        layers.append((f'linear{i}', nn.Linear(size, size)))\n",
    "        layers.append((f'activation{i}', activation))\n",
    "\n",
    "    layers.extend([(f'linear{n_layers+1}', nn.Linear(size, output_size)), (f'activation{n_layers+1}', output_activation)])\n",
    "\n",
    "    model = nn.Sequential(OrderedDict(layers))\n",
    "    print(model)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a17a32",
   "metadata": {},
   "source": [
    "# an abstract class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dd604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePolicy(object, metaclass=abc.ABCMeta):\n",
    "    def get_action(self, obs: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update(self, obs: np.ndarray, acs: np.ndarray, **kwargs) -> dict:\n",
    "        \"\"\"Return a dictionary of logging information.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, filepath: str):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb7348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPolicy(BasePolicy, nn.Module, metaclass=abc.ABCMeta):\n",
    "\n",
    "    def __init__(self,\n",
    "                 ac_dim,\n",
    "                 ob_dim,\n",
    "                 n_layers,\n",
    "                 size,\n",
    "                 discrete=False,\n",
    "                 learning_rate=1e-4,\n",
    "                 training=True,\n",
    "                 nn_baseline=False,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # init vars\n",
    "        self.ac_dim = ac_dim\n",
    "        self.ob_dim = ob_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.discrete = discrete\n",
    "        self.size = size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.training = training\n",
    "        self.nn_baseline = nn_baseline\n",
    "\n",
    "        self.logits_na = ptu.build_mlp(\n",
    "                input_size=self.ob_dim,\n",
    "                output_size=self.ac_dim,\n",
    "                n_layers=self.n_layers,\n",
    "                size=self.size,\n",
    "            )\n",
    "        \n",
    "        self.logits_na.to(ptu.device)\n",
    "        self.mean_net = None\n",
    "        self.logstd = None\n",
    "        self.optimizer = optim.Adam(self.logits_na.parameters(),\n",
    "                                        self.learning_rate)\n",
    "\n",
    "    ##################################\n",
    "\n",
    "    def save(self, filepath):\n",
    "        torch.save(self.state_dict(), filepath)\n",
    "\n",
    "    ##################################\n",
    "\n",
    "    def get_action(self, obs: np.ndarray) -> np.ndarray:  # -> is just a notation, you can delete it if you like\n",
    "        if len(obs.shape) > 1:\n",
    "            observation = obs\n",
    "        else:\n",
    "            observation = obs[None]\n",
    "\n",
    "        # TODO return the action that the policy prescribes\n",
    "        return ptu.to_numpy(self.forward(ptu.from_numpy(observation)))\n",
    "\n",
    "    # update/train this policy\n",
    "    def update(self, observations, actions, **kwargs):\n",
    "        \n",
    "        ################################\n",
    "        # implemented in child class, for better generality\n",
    "        # no loss function cannot implement update() function\n",
    "        ################################\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, observation: torch.FloatTensor):\n",
    "        \n",
    "        return self.mean_net(observation)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c831ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPolicySL(MLPPolicy):\n",
    "    def __init__(self, ac_dim, ob_dim, n_layers, size, **kwargs):\n",
    "        super().__init__(ac_dim, ob_dim, n_layers, size, **kwargs)\n",
    "        ################################\n",
    "        # In this case, the network doesn't have a loss function\n",
    "        # the net work structure just successed from father class(MLPPolicySL) \n",
    "        ################################\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def update( self, observations, actions, adv_n=None, acs_labels_na=None, qvals=None):\n",
    "        \n",
    "        ################################\n",
    "        # Just one round\n",
    "        ################################\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        pred = self.forward(ptu.from_numpy(observations)) # observations = input_data\n",
    "\n",
    "        loss = self.loss.forward(pred , ptu.from_numpy(action)) # action = label\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return {\n",
    "            # You can add extra logging information here, but keep this line\n",
    "            'Training Loss': ptu.to_numpy(loss),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32515c50",
   "metadata": {},
   "source": [
    "# MLP policy is built in BCAgent which was created in main(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52846b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCAgent(BaseAgent):\n",
    "    def __init__(self, env, agent_params):\n",
    "        super(BCAgent, self).__init__()\n",
    "\n",
    "        # init vars\n",
    "        self.env = env\n",
    "        self.agent_params = agent_params\n",
    "\n",
    "        # actor/policy\n",
    "        self.actor = MLPPolicySL(\n",
    "            self.agent_params['ac_dim'],\n",
    "            self.agent_params['ob_dim'],\n",
    "            self.agent_params['n_layers'],\n",
    "            self.agent_params['size'],\n",
    "            discrete=self.agent_params['discrete'],\n",
    "            learning_rate=self.agent_params['learning_rate'],\n",
    "        )\n",
    "    def train(self, ob_no, ac_na, re_n, next_ob_no, terminal_n):\n",
    "        # training a BC agent refers to updating its actor using\n",
    "        # the given observations and corresponding action labels\n",
    "        log = self.actor.update(ob_no, ac_na)  # HW1: you will modify this\n",
    "        return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7026d6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In trainer, trainer calls BCagent to train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4210aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(self):\n",
    "        print('\\nTraining agent using sampled data from replay buffer...')\n",
    "        all_logs = []\n",
    "        for train_step in range(self.params['num_agent_train_steps_per_iter']):\n",
    "\n",
    "            ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch = self.agent.sample(self.params['train_batch_size'])\n",
    "\n",
    "            train_log = self.agent.train(ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch)\n",
    "            \n",
    "            if train_step % 2000 == 0:\n",
    "                print(train_log)\n",
    "            all_logs.append(train_log)\n",
    "        return all_logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
